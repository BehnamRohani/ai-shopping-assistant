{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam_Local\\AppData\\Local\\Temp\\ipykernel_8380\\652266606.py:18: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_to)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data/torob-turbo-stage2.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "# import tarfile\n",
    "# import os\n",
    "\n",
    "# def extract_tar_gz(file_path: str, extract_to: str = \".\"):\n",
    "#     \"\"\"\n",
    "#     Extracts a .tar.gz file to the specified directory.\n",
    "\n",
    "#     :param file_path: Path to the .tar.gz file\n",
    "#     :param extract_to: Directory to extract files to (default: current directory)\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(file_path):\n",
    "#         raise FileNotFoundError(f\"{file_path} does not exist\")\n",
    "\n",
    "#     # Ensure the extraction directory exists\n",
    "#     os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "#     with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "#         tar.extractall(path=extract_to)\n",
    "#         print(f\"Extracted {file_path} to {extract_to}\")\n",
    "\n",
    "\n",
    "# extract_tar_gz(\"data/torob-turbo-stage2.tar.gz\", \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8084007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File: base_products.parquet ===\n",
      "Number of rows: 1022298\n",
      "Number of columns: 8\n",
      "Column names: ['image_url', 'random_key', 'category_id', 'brand_id', 'english_name', 'persian_name', 'extra_features', 'members']\n",
      "Data types:\n",
      " image_url         object\n",
      "random_key        object\n",
      "category_id        int64\n",
      "brand_id           int64\n",
      "english_name      object\n",
      "persian_name      object\n",
      "extra_features    object\n",
      "members           object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "                                           image_url random_key  category_id  \\\n",
      "0  https://image.torob.com/base/images/K1/aU/K1aU...     chjknu          338   \n",
      "1  https://image.torob.com/base/images/vx/gL/vxgL...     ppndkv         2453   \n",
      "2  https://image.torob.com/base/images/KW/2K/KW2K...     lwtjlj         2300   \n",
      "3  https://image.torob.com/base/images/VK/R5/VKR5...     ruqoum         1905   \n",
      "4  https://image.torob.com/base/images/eN/fh/eNfh...     gspcjx          350   \n",
      "\n",
      "   brand_id       english_name  \\\n",
      "0        -1                      \n",
      "1        -1            Ú†Ù†Ø¯ Ø±Ù†Ú¯   \n",
      "2      1293                      \n",
      "3       212                      \n",
      "4        -1  DESK yazdan MODEL   \n",
      "\n",
      "                                        persian_name  \\\n",
      "0                        ÙØ±Ø´ 700Ø´Ø§Ù†Ù‡ Ø§ÙÙ¾Ø§Ù„ ÙÛŒØ±ÙˆØ²Ù‡ Ø§ÛŒ   \n",
      "1                               ØµÙ†Ø¯Ù„ÛŒ ØªØ§Ø¨ Ù…Ø¯Ù„ chioco   \n",
      "2  Ø¨Ø´Ù‚Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÛŒÚ© ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡Ø³Ø§Ø²ÛŒ Ø¨Ø§ Ø³ÛŒØ³ØªÙ…Ø¹Ø§Ù…Ù„ 13 Ù‚...   \n",
      "3                   Ø³Ø±ÙˆÛŒØ³ Ù„Ú¯Ù† Ú¯Ø±Ø¯ ÛŒÙˆÙ†ÛŒÚ© Ø§ØµÙ„ - 6 Ø¹Ø¯Ø¯ÛŒ   \n",
      "4                        Ù…ÛŒØ² ØªØ­Ø±ÛŒØ± Ú†ÙˆØ¨ ÙÙ„Ø² Ù…Ø¯Ù„ ÛŒØ²Ø¯Ø§Ù†   \n",
      "\n",
      "                                      extra_features  \\\n",
      "0  {\"meterage\": [\"12 m\", \"9 m\", \"6 m\"], \"original...   \n",
      "1         {\"originality\": \"\", \"stock_status\": \"new\"}   \n",
      "2  {\"size2\": \"\", \"originality\": \"\", \"piece_count\"...   \n",
      "3  {\"originality\": \"\", \"piece_count\": \"6 pieces\",...   \n",
      "4  {\"material\": \"mdf\", \"originality\": \"\", \"stock_...   \n",
      "\n",
      "                                             members  \n",
      "0                                           [lqtdbp]  \n",
      "1                                           [iufoou]  \n",
      "2                                           [tjokxz]  \n",
      "3                                           [aysgod]  \n",
      "4  [rrzqgz, tfaizo, pgsmqi, vhbeny, mikqgh, chgee...   \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: base_views.parquet ===\n",
      "Number of rows: 199916\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'search_id', 'base_product_rk', 'timestamp']\n",
      "Data types:\n",
      " id                              object\n",
      "search_id                       object\n",
      "base_product_rk                 object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id search_id base_product_rk                        timestamp\n",
      "0  bchpng    jhemij          kayrjk 2025-09-13 00:00:16.739000+00:00\n",
      "1  wtbife    cxgzds          qrlxbr 2025-09-13 00:00:16.890000+00:00\n",
      "2  ayxzew    mjlzoz          ovnibg 2025-09-13 00:00:16.940000+00:00\n",
      "3  nitqjj    ovhikb          ojmhlz 2025-09-13 00:00:19.377000+00:00\n",
      "4  jpfagr    lmeylm          ahyiyw 2025-09-13 00:00:20.240000+00:00 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: brands.parquet ===\n",
      "Number of rows: 2025\n",
      "Number of columns: 2\n",
      "Column names: ['id', 'title']\n",
      "Data types:\n",
      " id        int64\n",
      "title    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "      id                 title\n",
      "0  54098     Ø¬ÛŒÙ…ÛŒÙ„Ø§ÛŒ / Gemilai\n",
      "1   2737      Ø¨ÙˆÙØ§Ù„Ùˆ / Buffalo\n",
      "2  54070          Ù…Ú©Ø³Ù† / Maxen\n",
      "3  54068         Ù†Ú©Ø³Ø§Ø± / Nexar\n",
      "4  54069  Ù†ÙˆØ±Ø³Ø§Ø²Ø§Ù† / Noorsazan \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: categories.parquet ===\n",
      "Number of rows: 746\n",
      "Number of columns: 3\n",
      "Column names: ['id', 'title', 'parent_id']\n",
      "Data types:\n",
      " id            int64\n",
      "title        object\n",
      "parent_id     int64\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "     id               title  parent_id\n",
      "0   212               ÛŒØ®Ú†Ø§Ù„       2461\n",
      "1   221           Ú©ÙˆÙ„Ø± Ú¯Ø§Ø²ÛŒ       2527\n",
      "2   338          ÙØ±Ø´ Ù…Ø§Ø´ÛŒÙ†ÛŒ       1872\n",
      "3   354  ØªØ´Ú©ØŒ Ù„Ø­Ø§Ù Ùˆ Ø±ÙˆØªØ®ØªÛŒ       2676\n",
      "4  1304   Ù‚Ù‡ÙˆÙ‡ Ùˆ Ø§Ø³Ù¾Ø±Ø³Ùˆ Ø³Ø§Ø²       1309 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: cities.parquet ===\n",
      "Number of rows: 651\n",
      "Number of columns: 2\n",
      "Column names: ['id', 'name']\n",
      "Data types:\n",
      " id       int64\n",
      "name    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "    id      name\n",
      "0  566      Ø²Ø±Ù†Ø¯\n",
      "1  299    Ú©Ù‡Ø±ÛŒØ²Ú©\n",
      "2   87    Ø§Ø±ÙˆÙ…ÛŒÙ‡\n",
      "3  210  Ø²Ø±ÛŒÙ† Ø±ÙˆØ¯\n",
      "4  153  Ø³Ø§ÙˆØ¬Ø¨Ù„Ø§Øº \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: final_clicks.parquet ===\n",
      "Number of rows: 17371\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'shop_id', 'base_view_id', 'timestamp']\n",
      "Data types:\n",
      " id                           object\n",
      "shop_id                       int64\n",
      "base_view_id                 object\n",
      "timestamp       datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id  shop_id base_view_id                        timestamp\n",
      "0  zmyxvb    23019       jrdkzd 2025-09-13 00:00:41.709975+00:00\n",
      "1  nmimcj     6013       cjmvyg 2025-09-13 00:00:45.498669+00:00\n",
      "2  azhuzt    19100       fwcfcw 2025-09-13 00:00:53.598091+00:00\n",
      "3  vaetkg    17458       xeujbq 2025-09-13 00:01:32.881068+00:00\n",
      "4  eiimms    22185       ucdxaa 2025-09-13 00:02:05.124268+00:00 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: members.parquet ===\n",
      "Number of rows: 1948665\n",
      "Number of columns: 4\n",
      "Column names: ['random_key', 'shop_id', 'price', 'base_random_key']\n",
      "Data types:\n",
      " random_key         object\n",
      "shop_id             int64\n",
      "price               int64\n",
      "base_random_key    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "  random_key  shop_id     price base_random_key\n",
      "0     lqtdbp     1314  17000000          chjknu\n",
      "1     iufoou    10370    246000          ppndkv\n",
      "2     tjokxz    23294   6950000          lwtjlj\n",
      "3     aysgod    22496   1850000          ruqoum\n",
      "4     rrzqgz    15837   6000000          gspcjx \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: searches.parquet ===\n",
      "Number of rows: 588347\n",
      "Number of columns: 9\n",
      "Column names: ['id', 'uid', 'page', 'session_id', 'timestamp', 'query', 'result_base_product_rks', 'category_id', 'category_brand_boosts']\n",
      "Data types:\n",
      " id                                      object\n",
      "uid                                     object\n",
      "page                                     int64\n",
      "session_id                              object\n",
      "timestamp                  datetime64[ns, UTC]\n",
      "query                                   object\n",
      "result_base_product_rks                 object\n",
      "category_id                              int64\n",
      "category_brand_boosts                   object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id     uid  page session_id                        timestamp  \\\n",
      "0  jhemij  lqnsla     0     jyhxbg 2025-09-13 00:00:00.133000+00:00   \n",
      "1  wvrviu  eghedv     0     ytenxp 2025-09-13 00:00:00.675000+00:00   \n",
      "2  oakxdu  yxcbhr     4     cyciuz 2025-09-13 00:00:01.672000+00:00   \n",
      "3  trnkhs  wzktjg     2     mdwxin 2025-09-13 00:00:02.021000+00:00   \n",
      "4  jmhhdq  vweoni     0     jtjwfu 2025-09-13 00:00:02.762000+00:00   \n",
      "\n",
      "                           query  \\\n",
      "0  ÛŒØ®Ú†Ø§Ù„ ÙØ±ÛŒØ²Ø± Ø·Ø±Ø­ Ø³Ø§ÛŒØ¯ Ù¾Ø§Ú©Ø´ÙˆÙ…Ø§    \n",
      "1                          ÛŒØ®Ú†Ø§Ù„   \n",
      "2                         Ø²ÙˆØ¯ Ù¾Ø²   \n",
      "3                         Ù…Ø¨Ù„Ù…Ø§Ù†   \n",
      "4                     Ù¾ØªÙˆ Ú¯Ù„Ø¨Ø§ÙØª   \n",
      "\n",
      "                             result_base_product_rks  category_id  \\\n",
      "0  [wdekwx, kayrjk, axmhow, pusbts, wdekwx, uyyyj...            0   \n",
      "1  [vkjhbx, ijfyrp, cvwrbk, lsmvew, tnbxnb, rwovy...            0   \n",
      "2  [qxnmle, utrrvh, ahmjqs, xsbzhe, irdbkp, vkbgi...            0   \n",
      "3  [pqzcom, niudms, tnymsy, rhyjpd, jjdwvw, nrhld...            0   \n",
      "4  [iryfsr, cvtxaa, rjiukp, ffjioj, kidjya, lydvk...            0   \n",
      "\n",
      "  category_brand_boosts  \n",
      "0                    []  \n",
      "1                    []  \n",
      "2                    []  \n",
      "3                    []  \n",
      "4                    []   \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: shops.parquet ===\n",
      "Number of rows: 23342\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'city_id', 'score', 'has_warranty']\n",
      "Data types:\n",
      " id                int64\n",
      "city_id           int64\n",
      "score           float64\n",
      "has_warranty       bool\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "      id  city_id  score  has_warranty\n",
      "0   1314      566    5.0         False\n",
      "1  10370      299    5.0         False\n",
      "2  23294       87    4.9         False\n",
      "3  22496      210    5.0         False\n",
      "4  15837      153    5.0          True \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Folder containing Parquet files\n",
    "data_folder = \"data/\"\n",
    "\n",
    "# Get all Parquet files in the folder\n",
    "parquet_files = glob.glob(os.path.join(data_folder, \"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(\"No Parquet files found in the data folder\")\n",
    "\n",
    "# Iterate through each file\n",
    "for f in parquet_files:\n",
    "    print(f\"=== File: {os.path.basename(f)} ===\")\n",
    "    \n",
    "    # Load the Parquet file\n",
    "    df = pd.read_parquet(f)\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "    print(\"Data types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nSample rows:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "    \n",
    "    print(\"-\" * 50, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bf48154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql, OperationalError\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = {\n",
    "    \"host\": \"81.12.30.45\",\n",
    "    \"port\": 29616,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"80aEx2MQtiWSlrWmPB16\"\n",
    "}\n",
    "\n",
    "def create_connection(config):\n",
    "    \"\"\"Create a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(**config)\n",
    "        print(\"Connection successful\")\n",
    "        return connection\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "        return None\n",
    "\n",
    "def close_connection(connection):\n",
    "    \"\"\"Close the database connection.\"\"\"\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Connection closed\")\n",
    "\n",
    "# Example usage\n",
    "conn = create_connection(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d462f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_parquet_file(file_path):\n",
    "    \"\"\"Read a Parquet file into a DataFrame.\"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"Loaded {len(df)} rows and {len(df.columns)} columns from {file_path}\")\n",
    "    return df\n",
    "def infer_postgres_column_types(df):\n",
    "    \"\"\"Infer PostgreSQL column types based on pandas dtypes.\"\"\"\n",
    "    column_defs = []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if pd.api.types.is_object_dtype(dtype):\n",
    "            column_type = \"TEXT\"\n",
    "        elif pd.api.types.is_integer_dtype(dtype):\n",
    "            column_type = \"BIGINT\"\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            column_type = \"FLOAT\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            column_type = \"TIMESTAMP WITH TIME ZONE\"\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            column_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            column_type = \"TEXT\"\n",
    "        column_defs.append(f\"{col} {column_type}\")\n",
    "    return column_defs\n",
    "def create_table(connection, table_name, column_defs):\n",
    "    \"\"\"Create a table if it does not exist.\"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(column_defs)});\"\n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    print(f\"Table '{table_name}' created or already exists.\")\n",
    "from tqdm import tqdm\n",
    "def insert_dataframe(connection, df, table_name, batch_size=1000):\n",
    "    \"\"\"Insert a pandas DataFrame into PostgreSQL in batches.\"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    cols = df.columns.tolist()\n",
    "    values_template = \", \".join([\"%s\"] * len(cols))\n",
    "\n",
    "    insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "        sql.Identifier(table_name),\n",
    "        sql.SQL(\", \").join(map(sql.Identifier, cols)),\n",
    "        sql.SQL(values_template)\n",
    "    )\n",
    "\n",
    "    data_tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    total_rows = len(data_tuples)\n",
    "\n",
    "    for i in tqdm(range(0, total_rows, batch_size)):\n",
    "        batch = data_tuples[i:i + batch_size]\n",
    "        cursor.executemany(insert_query, batch)\n",
    "        connection.commit()\n",
    "        # print(f\"Inserted rows {i}â€“{i+len(batch)-1} into '{table_name}'\")\n",
    "\n",
    "    cursor.close()\n",
    "    print(f\"âœ… Finished inserting {total_rows} rows into '{table_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ac8519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2025 rows and 2 columns from data/brands.parquet\n",
      "Table 'brands' created or already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [01:23<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished inserting 2025 rows into 'brands'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pname = \"brands\"\n",
    "file_path = f\"data/{pname}.parquet\"\n",
    "df = read_parquet_file(file_path)\n",
    "cols_sql = infer_postgres_column_types(df)\n",
    "create_table(conn, pname, cols_sql)\n",
    "insert_dataframe(conn, df, pname, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading data\\searches.parquet â†’ searches\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Mapping pandas dtypes â†’ PostgreSQL\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"BIGINT\",\n",
    "    \"float64\": \"DOUBLE PRECISION\",\n",
    "    \"bool\": \"BOOLEAN\",\n",
    "    \"datetime64[ns]\": \"TIMESTAMP\",\n",
    "    \"datetime64[ns, UTC]\": \"TIMESTAMPTZ\",\n",
    "    \"object\": \"TEXT\"\n",
    "}\n",
    "\n",
    "def map_dtype(dtype):\n",
    "    \"\"\"Convert pandas dtype â†’ PostgreSQL dtype\"\"\"\n",
    "    dtype_str = str(dtype)\n",
    "    return dtype_mapping.get(dtype_str, \"TEXT\")\n",
    "\n",
    "# Connect once\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        table_name = os.path.splitext(file)[0]  # filename = table name\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        \n",
    "        if table_name not in ['searches']:\n",
    "            continue\n",
    "        print(f\"ğŸ“¥ Loading {file_path} â†’ {table_name}\")\n",
    "\n",
    "        # Read parquet file into DataFrame\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # --- Step 1: Drop & recreate table with proper types ---\n",
    "        cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\" CASCADE;')\n",
    "        col_defs = \", \".join([\n",
    "            f'\"{col}\" {map_dtype(dtype)}' for col, dtype in zip(df.columns, df.dtypes)\n",
    "        ])\n",
    "        cursor.execute(f'CREATE TABLE \"{table_name}\" ({col_defs});')\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "        df.to_csv(buffer, index=False, header=False, sep=\",\", quoting=1)  # quoting=1 â†’ QUOTE_MINIMAL\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # --- Step 3: Bulk load with COPY CSV ---\n",
    "        cursor.copy_expert(f'COPY \"{table_name}\" FROM STDIN WITH CSV QUOTE \\'\"\\' DELIMITER \\',\\'', buffer)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"âœ… Inserted {len(df)} rows into {table_name}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"ğŸ‰ All parquet files loaded successfully with correct types!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71838a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam_Local\\AppData\\Local\\Temp\\ipykernel_7340\\3930754043.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "query = \"SELECT * FROM searches;\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb43725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to close the connection when done\n",
    "close_connection(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
