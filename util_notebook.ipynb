{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam_Local\\AppData\\Local\\Temp\\ipykernel_8380\\652266606.py:18: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_to)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data/torob-turbo-stage2.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "# import tarfile\n",
    "# import os\n",
    "\n",
    "# def extract_tar_gz(file_path: str, extract_to: str = \".\"):\n",
    "#     \"\"\"\n",
    "#     Extracts a .tar.gz file to the specified directory.\n",
    "\n",
    "#     :param file_path: Path to the .tar.gz file\n",
    "#     :param extract_to: Directory to extract files to (default: current directory)\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(file_path):\n",
    "#         raise FileNotFoundError(f\"{file_path} does not exist\")\n",
    "\n",
    "#     # Ensure the extraction directory exists\n",
    "#     os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "#     with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "#         tar.extractall(path=extract_to)\n",
    "#         print(f\"Extracted {file_path} to {extract_to}\")\n",
    "\n",
    "\n",
    "# extract_tar_gz(\"data/torob-turbo-stage2.tar.gz\", \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8084007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File: base_products.parquet ===\n",
      "Number of rows: 1022298\n",
      "Number of columns: 8\n",
      "Column names: ['image_url', 'random_key', 'category_id', 'brand_id', 'english_name', 'persian_name', 'extra_features', 'members']\n",
      "Data types:\n",
      " image_url         object\n",
      "random_key        object\n",
      "category_id        int64\n",
      "brand_id           int64\n",
      "english_name      object\n",
      "persian_name      object\n",
      "extra_features    object\n",
      "members           object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "                                           image_url random_key  category_id  \\\n",
      "0  https://image.torob.com/base/images/K1/aU/K1aU...     chjknu          338   \n",
      "1  https://image.torob.com/base/images/vx/gL/vxgL...     ppndkv         2453   \n",
      "2  https://image.torob.com/base/images/KW/2K/KW2K...     lwtjlj         2300   \n",
      "3  https://image.torob.com/base/images/VK/R5/VKR5...     ruqoum         1905   \n",
      "4  https://image.torob.com/base/images/eN/fh/eNfh...     gspcjx          350   \n",
      "\n",
      "   brand_id       english_name  \\\n",
      "0        -1                      \n",
      "1        -1            چند رنگ   \n",
      "2      1293                      \n",
      "3       212                      \n",
      "4        -1  DESK yazdan MODEL   \n",
      "\n",
      "                                        persian_name  \\\n",
      "0                        فرش 700شانه اُپال فیروزه ای   \n",
      "1                               صندلی تاب مدل chioco   \n",
      "2  بشقاب مجموعه کیک یکپارچهسازی با سیستمعامل 13 ق...   \n",
      "3                   سرویس لگن گرد یونیک اصل - 6 عددی   \n",
      "4                        میز تحریر چوب فلز مدل یزدان   \n",
      "\n",
      "                                      extra_features  \\\n",
      "0  {\"meterage\": [\"12 m\", \"9 m\", \"6 m\"], \"original...   \n",
      "1         {\"originality\": \"\", \"stock_status\": \"new\"}   \n",
      "2  {\"size2\": \"\", \"originality\": \"\", \"piece_count\"...   \n",
      "3  {\"originality\": \"\", \"piece_count\": \"6 pieces\",...   \n",
      "4  {\"material\": \"mdf\", \"originality\": \"\", \"stock_...   \n",
      "\n",
      "                                             members  \n",
      "0                                           [lqtdbp]  \n",
      "1                                           [iufoou]  \n",
      "2                                           [tjokxz]  \n",
      "3                                           [aysgod]  \n",
      "4  [rrzqgz, tfaizo, pgsmqi, vhbeny, mikqgh, chgee...   \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: base_views.parquet ===\n",
      "Number of rows: 199916\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'search_id', 'base_product_rk', 'timestamp']\n",
      "Data types:\n",
      " id                              object\n",
      "search_id                       object\n",
      "base_product_rk                 object\n",
      "timestamp          datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id search_id base_product_rk                        timestamp\n",
      "0  bchpng    jhemij          kayrjk 2025-09-13 00:00:16.739000+00:00\n",
      "1  wtbife    cxgzds          qrlxbr 2025-09-13 00:00:16.890000+00:00\n",
      "2  ayxzew    mjlzoz          ovnibg 2025-09-13 00:00:16.940000+00:00\n",
      "3  nitqjj    ovhikb          ojmhlz 2025-09-13 00:00:19.377000+00:00\n",
      "4  jpfagr    lmeylm          ahyiyw 2025-09-13 00:00:20.240000+00:00 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: brands.parquet ===\n",
      "Number of rows: 2025\n",
      "Number of columns: 2\n",
      "Column names: ['id', 'title']\n",
      "Data types:\n",
      " id        int64\n",
      "title    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "      id                 title\n",
      "0  54098     جیمیلای / Gemilai\n",
      "1   2737      بوفالو / Buffalo\n",
      "2  54070          مکسن / Maxen\n",
      "3  54068         نکسار / Nexar\n",
      "4  54069  نورسازان / Noorsazan \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: categories.parquet ===\n",
      "Number of rows: 746\n",
      "Number of columns: 3\n",
      "Column names: ['id', 'title', 'parent_id']\n",
      "Data types:\n",
      " id            int64\n",
      "title        object\n",
      "parent_id     int64\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "     id               title  parent_id\n",
      "0   212               یخچال       2461\n",
      "1   221           کولر گازی       2527\n",
      "2   338          فرش ماشینی       1872\n",
      "3   354  تشک، لحاف و روتختی       2676\n",
      "4  1304   قهوه و اسپرسو ساز       1309 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: cities.parquet ===\n",
      "Number of rows: 651\n",
      "Number of columns: 2\n",
      "Column names: ['id', 'name']\n",
      "Data types:\n",
      " id       int64\n",
      "name    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "    id      name\n",
      "0  566      زرند\n",
      "1  299    کهریزک\n",
      "2   87    ارومیه\n",
      "3  210  زرین رود\n",
      "4  153  ساوجبلاغ \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: final_clicks.parquet ===\n",
      "Number of rows: 17371\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'shop_id', 'base_view_id', 'timestamp']\n",
      "Data types:\n",
      " id                           object\n",
      "shop_id                       int64\n",
      "base_view_id                 object\n",
      "timestamp       datetime64[ns, UTC]\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id  shop_id base_view_id                        timestamp\n",
      "0  zmyxvb    23019       jrdkzd 2025-09-13 00:00:41.709975+00:00\n",
      "1  nmimcj     6013       cjmvyg 2025-09-13 00:00:45.498669+00:00\n",
      "2  azhuzt    19100       fwcfcw 2025-09-13 00:00:53.598091+00:00\n",
      "3  vaetkg    17458       xeujbq 2025-09-13 00:01:32.881068+00:00\n",
      "4  eiimms    22185       ucdxaa 2025-09-13 00:02:05.124268+00:00 \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: members.parquet ===\n",
      "Number of rows: 1948665\n",
      "Number of columns: 4\n",
      "Column names: ['random_key', 'shop_id', 'price', 'base_random_key']\n",
      "Data types:\n",
      " random_key         object\n",
      "shop_id             int64\n",
      "price               int64\n",
      "base_random_key    object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "  random_key  shop_id     price base_random_key\n",
      "0     lqtdbp     1314  17000000          chjknu\n",
      "1     iufoou    10370    246000          ppndkv\n",
      "2     tjokxz    23294   6950000          lwtjlj\n",
      "3     aysgod    22496   1850000          ruqoum\n",
      "4     rrzqgz    15837   6000000          gspcjx \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: searches.parquet ===\n",
      "Number of rows: 588347\n",
      "Number of columns: 9\n",
      "Column names: ['id', 'uid', 'page', 'session_id', 'timestamp', 'query', 'result_base_product_rks', 'category_id', 'category_brand_boosts']\n",
      "Data types:\n",
      " id                                      object\n",
      "uid                                     object\n",
      "page                                     int64\n",
      "session_id                              object\n",
      "timestamp                  datetime64[ns, UTC]\n",
      "query                                   object\n",
      "result_base_product_rks                 object\n",
      "category_id                              int64\n",
      "category_brand_boosts                   object\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "       id     uid  page session_id                        timestamp  \\\n",
      "0  jhemij  lqnsla     0     jyhxbg 2025-09-13 00:00:00.133000+00:00   \n",
      "1  wvrviu  eghedv     0     ytenxp 2025-09-13 00:00:00.675000+00:00   \n",
      "2  oakxdu  yxcbhr     4     cyciuz 2025-09-13 00:00:01.672000+00:00   \n",
      "3  trnkhs  wzktjg     2     mdwxin 2025-09-13 00:00:02.021000+00:00   \n",
      "4  jmhhdq  vweoni     0     jtjwfu 2025-09-13 00:00:02.762000+00:00   \n",
      "\n",
      "                           query  \\\n",
      "0  یخچال فریزر طرح ساید پاکشوما    \n",
      "1                          یخچال   \n",
      "2                         زود پز   \n",
      "3                         مبلمان   \n",
      "4                     پتو گلبافت   \n",
      "\n",
      "                             result_base_product_rks  category_id  \\\n",
      "0  [wdekwx, kayrjk, axmhow, pusbts, wdekwx, uyyyj...            0   \n",
      "1  [vkjhbx, ijfyrp, cvwrbk, lsmvew, tnbxnb, rwovy...            0   \n",
      "2  [qxnmle, utrrvh, ahmjqs, xsbzhe, irdbkp, vkbgi...            0   \n",
      "3  [pqzcom, niudms, tnymsy, rhyjpd, jjdwvw, nrhld...            0   \n",
      "4  [iryfsr, cvtxaa, rjiukp, ffjioj, kidjya, lydvk...            0   \n",
      "\n",
      "  category_brand_boosts  \n",
      "0                    []  \n",
      "1                    []  \n",
      "2                    []  \n",
      "3                    []  \n",
      "4                    []   \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "=== File: shops.parquet ===\n",
      "Number of rows: 23342\n",
      "Number of columns: 4\n",
      "Column names: ['id', 'city_id', 'score', 'has_warranty']\n",
      "Data types:\n",
      " id                int64\n",
      "city_id           int64\n",
      "score           float64\n",
      "has_warranty       bool\n",
      "dtype: object\n",
      "\n",
      "Sample rows:\n",
      "      id  city_id  score  has_warranty\n",
      "0   1314      566    5.0         False\n",
      "1  10370      299    5.0         False\n",
      "2  23294       87    4.9         False\n",
      "3  22496      210    5.0         False\n",
      "4  15837      153    5.0          True \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Folder containing Parquet files\n",
    "data_folder = \"data/\"\n",
    "\n",
    "# Get all Parquet files in the folder\n",
    "parquet_files = glob.glob(os.path.join(data_folder, \"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(\"No Parquet files found in the data folder\")\n",
    "\n",
    "# Iterate through each file\n",
    "for f in parquet_files:\n",
    "    print(f\"=== File: {os.path.basename(f)} ===\")\n",
    "    \n",
    "    # Load the Parquet file\n",
    "    df = pd.read_parquet(f)\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "    print(\"Data types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nSample rows:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "    \n",
    "    print(\"-\" * 50, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bf48154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql, OperationalError\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = {\n",
    "    \"host\": \"81.12.30.45\",\n",
    "    \"port\": 29616,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"80aEx2MQtiWSlrWmPB16\"\n",
    "}\n",
    "\n",
    "def create_connection(config):\n",
    "    \"\"\"Create a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(**config)\n",
    "        print(\"Connection successful\")\n",
    "        return connection\n",
    "    except OperationalError as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "        return None\n",
    "\n",
    "def close_connection(connection):\n",
    "    \"\"\"Close the database connection.\"\"\"\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Connection closed\")\n",
    "\n",
    "# Example usage\n",
    "conn = create_connection(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d462f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_parquet_file(file_path):\n",
    "    \"\"\"Read a Parquet file into a DataFrame.\"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"Loaded {len(df)} rows and {len(df.columns)} columns from {file_path}\")\n",
    "    return df\n",
    "def infer_postgres_column_types(df):\n",
    "    \"\"\"Infer PostgreSQL column types based on pandas dtypes.\"\"\"\n",
    "    column_defs = []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if pd.api.types.is_object_dtype(dtype):\n",
    "            column_type = \"TEXT\"\n",
    "        elif pd.api.types.is_integer_dtype(dtype):\n",
    "            column_type = \"BIGINT\"\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            column_type = \"FLOAT\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            column_type = \"TIMESTAMP WITH TIME ZONE\"\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            column_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            column_type = \"TEXT\"\n",
    "        column_defs.append(f\"{col} {column_type}\")\n",
    "    return column_defs\n",
    "def create_table(connection, table_name, column_defs):\n",
    "    \"\"\"Create a table if it does not exist.\"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(column_defs)});\"\n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    print(f\"Table '{table_name}' created or already exists.\")\n",
    "from tqdm import tqdm\n",
    "def insert_dataframe(connection, df, table_name, batch_size=1000):\n",
    "    \"\"\"Insert a pandas DataFrame into PostgreSQL in batches.\"\"\"\n",
    "    cursor = connection.cursor()\n",
    "    cols = df.columns.tolist()\n",
    "    values_template = \", \".join([\"%s\"] * len(cols))\n",
    "\n",
    "    insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "        sql.Identifier(table_name),\n",
    "        sql.SQL(\", \").join(map(sql.Identifier, cols)),\n",
    "        sql.SQL(values_template)\n",
    "    )\n",
    "\n",
    "    data_tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    total_rows = len(data_tuples)\n",
    "\n",
    "    for i in tqdm(range(0, total_rows, batch_size)):\n",
    "        batch = data_tuples[i:i + batch_size]\n",
    "        cursor.executemany(insert_query, batch)\n",
    "        connection.commit()\n",
    "        # print(f\"Inserted rows {i}–{i+len(batch)-1} into '{table_name}'\")\n",
    "\n",
    "    cursor.close()\n",
    "    print(f\"✅ Finished inserting {total_rows} rows into '{table_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ac8519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2025 rows and 2 columns from data/brands.parquet\n",
      "Table 'brands' created or already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [01:23<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished inserting 2025 rows into 'brands'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pname = \"brands\"\n",
    "file_path = f\"data/{pname}.parquet\"\n",
    "df = read_parquet_file(file_path)\n",
    "cols_sql = infer_postgres_column_types(df)\n",
    "create_table(conn, pname, cols_sql)\n",
    "insert_dataframe(conn, df, pname, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading data\\searches.parquet → searches\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Mapping pandas dtypes → PostgreSQL\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"BIGINT\",\n",
    "    \"float64\": \"DOUBLE PRECISION\",\n",
    "    \"bool\": \"BOOLEAN\",\n",
    "    \"datetime64[ns]\": \"TIMESTAMP\",\n",
    "    \"datetime64[ns, UTC]\": \"TIMESTAMPTZ\",\n",
    "    \"object\": \"TEXT\"\n",
    "}\n",
    "\n",
    "def map_dtype(dtype):\n",
    "    \"\"\"Convert pandas dtype → PostgreSQL dtype\"\"\"\n",
    "    dtype_str = str(dtype)\n",
    "    return dtype_mapping.get(dtype_str, \"TEXT\")\n",
    "\n",
    "# Connect once\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        table_name = os.path.splitext(file)[0]  # filename = table name\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        \n",
    "        if table_name not in ['searches']:\n",
    "            continue\n",
    "        print(f\"📥 Loading {file_path} → {table_name}\")\n",
    "\n",
    "        # Read parquet file into DataFrame\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # --- Step 1: Drop & recreate table with proper types ---\n",
    "        cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\" CASCADE;')\n",
    "        col_defs = \", \".join([\n",
    "            f'\"{col}\" {map_dtype(dtype)}' for col, dtype in zip(df.columns, df.dtypes)\n",
    "        ])\n",
    "        cursor.execute(f'CREATE TABLE \"{table_name}\" ({col_defs});')\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "        df.to_csv(buffer, index=False, header=False, sep=\",\", quoting=1)  # quoting=1 → QUOTE_MINIMAL\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # --- Step 3: Bulk load with COPY CSV ---\n",
    "        cursor.copy_expert(f'COPY \"{table_name}\" FROM STDIN WITH CSV QUOTE \\'\"\\' DELIMITER \\',\\'', buffer)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"✅ Inserted {len(df)} rows into {table_name}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"🎉 All parquet files loaded successfully with correct types!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71838a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam_Local\\AppData\\Local\\Temp\\ipykernel_7340\\3930754043.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "query = \"SELECT * FROM searches;\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb43725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to close the connection when done\n",
    "close_connection(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
